{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our attempt to recreate / reproduce the model described in https://github.com/YoungSeng/DiffuseStyleGesture\n",
    "This won the reproducability award in the 2023 Genea gesture animation challenge. This is the reason that we are reproducing it. \n",
    "\n",
    "\n",
    "Or more specifically, we are reproducing it to improve our understanding of the field and existing approaches. This is with the goal of creating a system of our own at the end.\n",
    "\n",
    "Learning goals are:\n",
    "- We will understand the structure of the data and how to work with it\n",
    "- We want to understand how to preprocess the data\n",
    "- We want build a gesture syntehesis ML model from scratch\n",
    "- We want to train a advanced ML model, which we have never done before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are unsure of what the best way to start is, but we will begin with transforming data from bvh to some numpy representation that we can do machine learning with, and back to bvh. DiffuseStyleGesture extract all sorts of extra information from the animation data, such as acceleration, rotational ecceleration, velocity, etc etc. We should probably try to do some of the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found and downloaded pymo. https://omid.al/projects/pymo/\n",
    "This is a library for doing machine learning with motion capture data. We are using files prepared for the genea challenge that make use of this library to convert from bvh to features (a numpy tensor)\n",
    "\n",
    "The script for the genea challenge extends the pipeline with a root normalisation step, which is intended to make sure all subjects are pointed the same way. For now we skip this step as we will start by just training on the main subject and not take the conversation partner into account.\n",
    "\n",
    "One idea we have gotten for data augmentation to increase the dataset size is to duplicate the dataset and mirror it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################\n",
    "THE PLAN\n",
    "#################\n",
    "\n",
    "We are planning what items we need to extract for our machine learning system to work\n",
    "\n",
    "We need to:\n",
    " - We need to sample from our motion capture data and extract a clip of lenth N + N_seed\n",
    "    - From this, we need to extract the \"true\" clip, the label, N frames\n",
    "    - And also, N_seed frames which will be fed to the system to help it generate continous animation with awareness of preceeding motion\n",
    " - We need to extract a corresponding audio snippet\n",
    "    - From this we want to extract\n",
    "        - Pitch\n",
    "        - Energy\n",
    "        - Onsets\n",
    "        - Mel spectrum\n",
    "        - MFCC\n",
    "        - WavLM\n",
    " - We need to extract Style / ID (one hot encoded) from the metadata\n",
    " - We need to sample uniformly a timestep t for our noising / diffusion / forward step. We copy the true gesture we sampled and add gaussian noise according to t.\n",
    " - Now we attempt to donoise this and compare with the true data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style_diffuse_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
