{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TEST 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to incorporate a transpose operation in a nn.Sequential operation, we make a 'TransposeLayer' as a nn.Module. \n",
    "\n",
    "This is needed because we can't just pass a matrix (2D tensor) to torch's liniar layers. Torch can work with matrixes, but are design to work with then in the context of multiple baches run in parallel.\n",
    "\n",
    "For that reason, we can't pass multipe collums to the liniar layer, but have to transpose these to become row vetors insted. We can then transpose them back to column vectors after the liniar operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overwrite the nn.Module from pytorch, and encapsulate the model's feed forward process, that can be used both during training and inferance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffuseStyleGestureRecModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                number_of_styles: int,                      # Number of unique styles. In this context this is the number of speakers, since we treat each speaker as a style\n",
    "                N_gesture_length: int,                      # Length of the sequence snippets to generate. We geneate in autoregressive manner, where we are constantly generating small chunks continously\n",
    "                N_seed_length: int,                         # Length of the seed that we use. The seed is the number of frames from the previously generated sequence in order to make the generation smooth continous  \n",
    "                audio_features_per_frame: int,              # Number of audio features per frame. This is a mixture of prosodic features, onsets, wavlm, etc.\n",
    "                pose_features_per_frame: int):              # Number of pose features per frame. These are the rotations / translations of the bones in the character skeleton. We may not pay attention to every channel for every bone, or every bone. \n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # Implemetation of the DiffuseStyleGestureModel based on the paper by YoungSeng et al.\n",
    "        # We instantiate all learned model layers needed below.\n",
    "\n",
    "        # The time step encoding MLP. Our best guess is that this is actually a learned position encoding as described in Vaswani et al. \n",
    "        # Maybe it could be interesting to investigate using sinosoidal positional encoding? That would be one way to reduce the number of weights and maybe it would run faster?\n",
    "        # Sinosoidal position embeddings seem to work very well, and in Andrej Kaparthy's llm video series he even investigates the gpt2 weights, and claim that they are not fully\n",
    "        # converged yet, because they are spiky.\n",
    "        self.time_step_mlp = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "        \n",
    "        # Seed gesture linear layer - for dimensionality reduction\n",
    "        # We reduce the seed gesture (pose_features_per_frame, N_seed_length) to be just 192. it sure seems like we are compressing the seed a lot. This is for 8 frames, and its barely\n",
    "        # enough data to store a 10th of a single frame! I guess it captures the broad tendencies in the previous motion. I hope it's enough to make the motion smooth.\n",
    "        self.seed_gesture_linear = nn.Linear(\n",
    "            in_features=pose_features_per_frame * N_seed_length, \n",
    "            out_features=192\n",
    "        )\n",
    "        # TODO: Add as nn.Sequential where we can flatten??\n",
    "\n",
    "        # Style linear layer - for dimensionality expansion from a onehot encoded (number_of_styles, 1) to (64, 1) shape\n",
    "        # We move from a one hot encoded format to a 64 dimensional vector. My best guess is that instead of working with individual styles we sort of extract features of the style.'\n",
    "        # For instance, 2 speakers might share the same general waviness in their gestures, and this can be encoded as a feature which is shared between the two speakers.\n",
    "        # This is a way to make the model more general, and to make it easier to generalize to new speakers.\n",
    "        self.style_linear = nn.Linear(\n",
    "            in_features=number_of_styles, \n",
    "            out_features=64\n",
    "        )\n",
    "        \n",
    "        # Audio feature linear layer per frame - for dimensionality reduction\n",
    "        # The idea is to reduce the number of audio features per frame to a much smaller number. I know that we use wavlm which produces a huge number of embeddings. \n",
    "        # From the illustration, it is not clear if only wavlm is passed through the layer, or other audio featuers as well. Something to investigate.\n",
    "        # TODO: think about transpose, I think we should do it directly in forward pass code stuff\n",
    "        self.audio_linear = nn.Linear(\n",
    "            in_features=audio_features_per_frame, \n",
    "            out_features=64\n",
    "        )\n",
    "\n",
    "        # Noisy gesture sequence linear layer - for dimensionality reduction\n",
    "        # Must be applied to each frame vector in the sequence tensor, individually\n",
    "        # Here we go from pose feature dimension (1141) to 256. I guess we are representing the pose in a more general way. \n",
    "        # Of course the vast majority of combinations of rotations of limbs are highly unlikely, so it makes some sense that we compress it a lot.\n",
    "        # Based on the idea of \"nice\" and \"ugly\" numbers which we got from Andrej Kaparthy, we are thinking that it might be good to change the number 1141. \n",
    "        # If we can get rid of / add some extra featuers to pay attention to it might make it faster.\n",
    "        self.noisy_gesture_linear = nn.Linear(\n",
    "            in_features=pose_features_per_frame, \n",
    "            out_features=256\n",
    "        )\n",
    "        # Attention layers\n",
    "        # TODO: Make number of heads and number of attention layers a hyper-parameter\n",
    "        \n",
    "        # TODO: Use the cross_local_attention function from the paper\n",
    "        self.cross_locale_attention = nn.MultiheadAttention(\n",
    "            embed_dim=256, \n",
    "            num_heads=8, \n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=256, \n",
    "            num_heads=8, \n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # Final transformation, creating the output\n",
    "        self.final_linear = nn.Linear(256, pose_features_per_frame)\n",
    "    \n",
    "    def forward(self, \n",
    "                frames_are_culumns: bool,\n",
    "\n",
    "                t_current_defusion_time_step:int,\n",
    "                seed_gesture_tensor, \n",
    "                one_hot_style_tensor, \n",
    "                audio_features_tensor, \n",
    "                noisy_gesture_sequence_tensor,\n",
    "\n",
    "                apply_random_mask_to_seed: bool, \n",
    "                apply_random_mask_to_style: bool):\n",
    "        \n",
    "        # 1.1 - Prepare the diffusion time step t\n",
    "\n",
    "        # 1.1.1 - Add positional encoding to the t (current time step) using a MLP\n",
    "        #       Producing a tensor of shape (256, 1)\n",
    "        t_tensor = self.time_step_mlp(t_current_defusion_time_step.view(-1, 1))  # (256, 1)\n",
    "        \n",
    "\n",
    "        # 1.2 - Prepare the seed gesture\n",
    "        \n",
    "        # 1.2.1 - mask the seed gesture if apply_random_mask_to_seed is True\n",
    "        if apply_random_mask_to_seed:\n",
    "            seed_gesture_tensor = torch.zeros_like(seed_gesture_tensor)\n",
    "        \n",
    "        # 1.2.2 - Apply a linear layer to get a tensor of shape (192, 1) \n",
    "        #         We reshape the seed gesture tensor before applying the linear layer\n",
    "        #         using view(-1) to flatten the tensor\n",
    "        seed_gesture_tensor = self.seed_gesture_linear(seed_gesture_tensor.flatten())  # (192, 1)\n",
    "\n",
    "        # 1.3 - Prepare the style tensor\n",
    "\n",
    "        # 1.3.1 - Apply a linear layer to get a tensor of shape (64, 1)\n",
    "        \n",
    "        style_tensor = self.style_linear(one_hot_style_tensor) \n",
    "\n",
    "        # 1.3.2 - Mask the style if apply_random_mask_to_style is True\n",
    "        if apply_random_mask_to_style:\n",
    "            style_tensor = torch.zeros_like(style_tensor)\n",
    "\n",
    "\n",
    "        # 1.4 - Prepare the audio features tensor\n",
    "        #       Apply a linear layer to get a tensor of shape (64, N) - every column is the features for that frame\n",
    "        #       TODO: Consider making this 128 to make the final tenser of shape (640, N) (Nice number)\n",
    "        audio_features_tensor = self.audio_linear(audio_features_tensor)\n",
    "        \n",
    "        # 1.5 - Prepare the noisy gesture sequence tensor (1141, N)\n",
    "        #       Apply a linear layer to get a tensor of shape (256, N) - every column is the features for that frame\n",
    "        #       The linear layer is applied per frame\n",
    "        noisy_gesture_sequence_tensor = self.noisy_gesture_linear(noisy_gesture_sequence_tensor)\n",
    "\n",
    "        # 2 - Combine input tensors to get the input tensor for the model\n",
    "\n",
    "        # 2.1 - Concatenate the transformed seed gesture tensor, style tensor\n",
    "        #       To get a tensor of shape (256, 1), called the seed_style_t_tensor\n",
    "        seed_style_t_tensor = torch.cat((seed_gesture_tensor, style_tensor), dim=0)\n",
    "        \n",
    "        # 2.1.1 - And add the time step tensor using element wise addition\n",
    "        seed_style_t_tensor += t_tensor\n",
    "\n",
    "        # 2.2 - Concatenate the audio features tensor and Noisy gesture sequence tensor\n",
    "        #       To get a tensor of shape (320, N)\n",
    "        audio_noisy_gesture_tensor = torch.cat([audio_features_tensor, noisy_gesture_sequence_tensor], dim=0)  # (320, N)\n",
    "        \n",
    "        # 2.3 - replicate the seed_style_t_tensor to get a tensor of shape (256, N)\n",
    "\n",
    "        # 2.4 - Concatenate the seed_style_t_tensor and the audio_features_noisy_gesture_sequence tensor\n",
    "        #       To get a tensor of shape (576, N) (Could be nicer)\n",
    "        #       This gives us the 'input_tensor' for the model\n",
    "\n",
    "\n",
    "        # 3 - The Attention layers\n",
    "\n",
    "        # 3.1 - Add RPE (Relative Positional Encoding) to the input_tensor\n",
    "\n",
    "        # 3.2 - apply Cross-Locale Attention to the RPE'ed input tensor\n",
    "\n",
    "        # 3.3 - Pass the output of the Cross-Locale Attention to a liniear layer \n",
    "        #       to get a tensor of shape (256, N)\n",
    "\n",
    "        # 3.4 - Concatenate the output of the liniear layer with the seed_style_t_tensor\n",
    "        #       To get a tensor of shape (256, N+1)\n",
    "\n",
    "        # 3.5 - Apply a self attention layer to the tensor of shape (256, N+1)\n",
    "\n",
    "        # 3.6 - Pass the output of the self attention layer to a liniear layer,\n",
    "        #       to get a tensor of shape (1141, N)\n",
    "\n",
    "        # 4 - Return the output of the liniear layer\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
